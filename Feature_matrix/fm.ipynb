{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import lfilter\n",
    "from spectrum import arburg\n",
    "import matplotlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jArithmeticMean(X, unused_param=None):\n",
    "    # Calculate the arithmetic mean\n",
    "    AM = np.mean(X)\n",
    "    return AM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meadian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jMedian(X):\n",
    "    # Median calculation\n",
    "    X_med = np.median(X)\n",
    "    return X_med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jVariance(X):\n",
    "    N = len(X)\n",
    "    mu = np.mean(X)\n",
    "    VAR = (1 / (N - 1)) * np.sum((X - mu) ** 2)\n",
    "    return VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standerd Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jStandardDeviation(X):\n",
    "    N = len(X)\n",
    "    mu = np.mean(X)\n",
    "    SD = np.sqrt((1 / (N - 1)) * np.sum((X - mu) ** 2))\n",
    "    return SD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "def jSkewness(X):\n",
    "    # Skewness calculation\n",
    "    SKEW = skew(X)\n",
    "    return SKEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "\n",
    "def jKurtosis(X):\n",
    "    # Kurtosis calculation\n",
    "    KURT = kurtosis(X)\n",
    "    return KURT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jMaximum(X):\n",
    "    # Maximum value calculation\n",
    "    X_max = max(X)\n",
    "    return X_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jMinimum(X):\n",
    "    # Minimum value calculation\n",
    "    X_min = min(X)\n",
    "    return X_min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hjorth Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jHjorthActivity(X):\n",
    "    sd = np.std(X)\n",
    "    HA = sd ** 2\n",
    "    return HA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hjorth Mobility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jHjorthMobility(X):\n",
    "    # First derivative\n",
    "    x0 = np.array(X)\n",
    "    x1 = np.diff(np.concatenate(([0], x0)))\n",
    "    \n",
    "    # Standard deviation\n",
    "    sd0 = np.std(x0)\n",
    "    sd1 = np.std(x1)\n",
    "    \n",
    "    # Mobility\n",
    "    HM = sd1 / sd0\n",
    "    \n",
    "    return HM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hjorth Complexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def jHjorthComplexity(X):\n",
    "    # First & second derivative\n",
    "    x0 = np.array(X)\n",
    "    x1 = np.diff(np.concatenate(([0], x0)))\n",
    "    x2 = np.diff(np.concatenate(([0], x1)))\n",
    "    \n",
    "    # Standard deviation of first & second derivative\n",
    "    sd0 = np.std(x0)\n",
    "    sd1 = np.std(x1)\n",
    "    sd2 = np.std(x2)\n",
    "    \n",
    "    # Complexity\n",
    "    HC = (sd2 / sd1) / (sd1 / sd0)\n",
    "    \n",
    "    return HC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Energy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jMeanEnergy(X):\n",
    "    # Mean energy calculation\n",
    "    ME = np.mean(np.square(X))\n",
    "    return ME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time domain Feature matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_for_one(x):\n",
    "    print(\"hello\")\n",
    "    \n",
    "\n",
    "\n",
    "df = pd.read_csv(f'./Final_data/subject_{1}/session_{2}_{\"BMT\"}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame()\n",
    "for i in range(0,1):\n",
    "    \n",
    "    for sub in range(0,1):\n",
    "        # print(i)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        for state in ['BMT','DMT']:\n",
    "            df = pd.read_csv(f'./dataset2_n/{state}.csv')\n",
    "            # get all column \n",
    "            columns = df.columns\n",
    "            # print(type(columns))\n",
    "            # print(columns)\n",
    "            index =0\n",
    "            size = 600 #3 second window \n",
    "            final_data =[]\n",
    "            while index<len(df[columns[0]]):\n",
    "                ftm = []\n",
    "                for cl in columns:\n",
    "                    # print(cl)\n",
    "                    lst = df[cl][index:index+size].tolist()\n",
    "                    # print(len(lst))\n",
    "                    if(len(lst)<200):\n",
    "                        break\n",
    "                    ft1=[]\n",
    "                    ft1.append(jArithmeticMean(lst))\n",
    "                    ft1.append(jVariance(lst))\n",
    "                    ft1.append(jMedian(lst))\n",
    "                    ft1.append(jStandardDeviation(lst))\n",
    "                    ft1.append(jMaximum(lst))\n",
    "                    ft1.append(jMinimum(lst))\n",
    "                    ft1.append(jSkewness(lst))\n",
    "                    ft1.append(kurtosis(lst))\n",
    "                    ft1.append(jHjorthActivity(lst))\n",
    "                    ft1.append(jHjorthComplexity(lst))\n",
    "                    ft1.append(jHjorthMobility(lst))\n",
    "                    ft1.append(jMeanEnergy(lst))\n",
    "                    ftm.append(ft1)\n",
    "                if len(ftm)!=0:\n",
    "                    # print(\"Ftm size\", len(ftm), len(ftm[0]))\n",
    "                    temp=[]\n",
    "                    for c in range(0,len(ftm[0])):\n",
    "                        for r in range(0, len(ftm)):\n",
    "                            temp.append(ftm[r][c])\n",
    "                    # print(\"print size of temp \",len(temp))\n",
    "                    if(state==\"BMT\"):\n",
    "                        temp.append(1)\n",
    "                    else:\n",
    "                        temp.append(0)\n",
    "                    # print(\"print size of temp \",len(temp))\n",
    "                    final_data.append(temp)\n",
    "                index = index + 200\n",
    "                # Convert final_data to a DataFrame\n",
    "            final_df = pd.DataFrame(final_data)\n",
    "            \n",
    "            # Append the DataFrame to the output DataFrame\n",
    "            output_df = pd.concat([output_df, final_df], ignore_index=True)\n",
    "            # print(\"finally dimention for 1 person \", len(final_data), len(final_data[0]))\n",
    "            \n",
    "\n",
    "    # print(sub)\n",
    "rows1, columns1 = output_df.shape\n",
    "print(rows1, columns1)\n",
    "output_df.to_csv('./dataset2_feature_mat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in ['BMT','DMT']:\n",
    "            df = pd.read_csv(f'./Final_data/subject_{sub}/session_{i*2}_{state}.csv')\n",
    "            # get all column \n",
    "            columns = df.columns\n",
    "            # print(type(columns))\n",
    "            # print(columns)\n",
    "            index =0\n",
    "            size = 600 #3 second window \n",
    "            final_data =[]\n",
    "            while index<len(df[columns[0]]):\n",
    "                ftm = []\n",
    "                for cl in columns:\n",
    "                    # print(cl)\n",
    "                    lst = df[cl][index:index+size].tolist()\n",
    "                    # print(len(lst))\n",
    "                    if(len(lst)<200):\n",
    "                        break\n",
    "                    ft1=[]\n",
    "                    ft1.append(jArithmeticMean(lst))\n",
    "                    ft1.append(jVariance(lst))\n",
    "                    ft1.append(jMedian(lst))\n",
    "                    ft1.append(jStandardDeviation(lst))\n",
    "                    ft1.append(jMaximum(lst))\n",
    "                    ft1.append(jMinimum(lst))\n",
    "                    ft1.append(jSkewness(lst))\n",
    "                    ft1.append(kurtosis(lst))\n",
    "                    ft1.append(jHjorthActivity(lst))\n",
    "                    ft1.append(jHjorthComplexity(lst))\n",
    "                    ft1.append(jHjorthMobility(lst))\n",
    "                    ft1.append(jMeanEnergy(lst))\n",
    "                    ftm.append(ft1)\n",
    "                if len(ftm)!=0:\n",
    "                    # print(\"Ftm size\", len(ftm), len(ftm[0]))\n",
    "                    temp=[]\n",
    "                    for c in range(0,len(ftm[0])):\n",
    "                        for r in range(0, len(ftm)):\n",
    "                            temp.append(ftm[r][c])\n",
    "                    # print(\"print size of temp \",len(temp))\n",
    "                    if(state==\"BMT\"):\n",
    "                        temp.append(1)\n",
    "                    else:\n",
    "                        temp.append(0)\n",
    "                    # print(\"print size of temp \",len(temp))\n",
    "                    final_data.append(temp)\n",
    "                index = index + 200\n",
    "                # Convert final_data to a DataFrame\n",
    "            final_df = pd.DataFrame(final_data)\n",
    "            \n",
    "            # Append the DataFrame to the output DataFrame\n",
    "            output_df = pd.concat([output_df, final_df], ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data from the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('./dataset2_feature_mat.csv')\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows containing NaN values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select the columns you want to normalize\n",
    "columns_to_normalize = data.columns\n",
    "new_columns = columns_to_normalize[2:]\n",
    "\n",
    "# Fit and transform the selected columns using MinMaxScaler\n",
    "data[new_columns] = scaler.fit_transform(data[new_columns])\n",
    "\n",
    "# Save the normalized data to a new CSV file\n",
    "data.to_csv('normalized_dataset2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature ranking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-square test statistics\n",
    "Top 50 important feature \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: 2 - Chi-square statistic: 7672.000000000002\n",
      "Rank 2: 6 - Chi-square statistic: 7672.000000000002\n",
      "Rank 3: 9 - Chi-square statistic: 7672.000000000002\n",
      "Rank 4: 17 - Chi-square statistic: 7672.000000000002\n",
      "Rank 5: 28 - Chi-square statistic: 7672.000000000002\n",
      "Rank 6: 50 - Chi-square statistic: 7672.000000000002\n",
      "Rank 7: 52 - Chi-square statistic: 7672.000000000002\n",
      "Rank 8: 53 - Chi-square statistic: 7672.000000000002\n",
      "Rank 9: 60 - Chi-square statistic: 7672.000000000002\n",
      "Rank 10: 65 - Chi-square statistic: 7672.000000000002\n",
      "Rank 11: 66 - Chi-square statistic: 7672.000000000002\n",
      "Rank 12: 95 - Chi-square statistic: 7672.000000000002\n",
      "Rank 13: 114 - Chi-square statistic: 7672.000000000002\n",
      "Rank 14: 116 - Chi-square statistic: 7672.000000000002\n",
      "Rank 15: 117 - Chi-square statistic: 7672.000000000002\n",
      "Rank 16: 124 - Chi-square statistic: 7672.000000000002\n",
      "Rank 17: 192 - Chi-square statistic: 7672.000000000002\n",
      "Rank 18: 202 - Chi-square statistic: 7672.000000000002\n",
      "Rank 19: 233 - Chi-square statistic: 7672.000000000002\n",
      "Rank 20: 274 - Chi-square statistic: 7672.000000000002\n",
      "Rank 21: 277 - Chi-square statistic: 7672.000000000002\n",
      "Rank 22: 282 - Chi-square statistic: 7672.000000000002\n",
      "Rank 23: 284 - Chi-square statistic: 7672.000000000002\n",
      "Rank 24: 289 - Chi-square statistic: 7672.000000000002\n",
      "Rank 25: 293 - Chi-square statistic: 7672.000000000002\n",
      "Rank 26: 294 - Chi-square statistic: 7672.000000000002\n",
      "Rank 27: 295 - Chi-square statistic: 7672.000000000002\n",
      "Rank 28: 296 - Chi-square statistic: 7672.000000000002\n",
      "Rank 29: 306 - Chi-square statistic: 7672.000000000002\n",
      "Rank 30: 311 - Chi-square statistic: 7672.000000000002\n",
      "Rank 31: 345 - Chi-square statistic: 7672.000000000002\n",
      "Rank 32: 349 - Chi-square statistic: 7672.000000000002\n",
      "Rank 33: 353 - Chi-square statistic: 7672.000000000002\n",
      "Rank 34: 369 - Chi-square statistic: 7672.000000000002\n",
      "Rank 35: 371 - Chi-square statistic: 7672.000000000002\n",
      "Rank 36: 377 - Chi-square statistic: 7672.000000000002\n",
      "Rank 37: 0 - Chi-square statistic: 7672.000000000001\n",
      "Rank 38: 1 - Chi-square statistic: 7672.000000000001\n",
      "Rank 39: 3 - Chi-square statistic: 7672.000000000001\n",
      "Rank 40: 5 - Chi-square statistic: 7672.000000000001\n",
      "Rank 41: 8 - Chi-square statistic: 7672.000000000001\n",
      "Rank 42: 10 - Chi-square statistic: 7672.000000000001\n",
      "Rank 43: 19 - Chi-square statistic: 7672.000000000001\n",
      "Rank 44: 20 - Chi-square statistic: 7672.000000000001\n",
      "Rank 45: 21 - Chi-square statistic: 7672.000000000001\n",
      "Rank 46: 22 - Chi-square statistic: 7672.000000000001\n",
      "Rank 47: 23 - Chi-square statistic: 7672.000000000001\n",
      "Rank 48: 24 - Chi-square statistic: 7672.000000000001\n",
      "Rank 49: 25 - Chi-square statistic: 7672.000000000001\n",
      "Rank 50: 26 - Chi-square statistic: 7672.000000000001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load your data from the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('normalized_data.csv')\n",
    "\n",
    "clmn = data.columns\n",
    "\n",
    "# Assuming your target variable is 'target_column' and your feature columns are in 'feature_columns'\n",
    "target_column = clmn[len(clmn)-1]\n",
    "feature_columns = clmn[:-1]\n",
    "\n",
    "# Create an empty dictionary to store chi-square statistics for each feature\n",
    "chi2_stats = {}\n",
    "\n",
    "# Loop through each feature column\n",
    "for feature in feature_columns:\n",
    "    # Create a contingency table between the current feature and the target variable\n",
    "    contingency_table = pd.crosstab(data[feature], data[target_column])\n",
    "    \n",
    "    # Perform the chi-square test and get the chi-square statistic, p-value, degrees of freedom, and expected frequencies\n",
    "    chi2_stat, _, _, _ = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Store the chi-square statistic in the dictionary\n",
    "    chi2_stats[feature] = chi2_stat\n",
    "\n",
    "# Sort the features based on their chi-square statistics in descending order\n",
    "sorted_features = sorted(chi2_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 50 important features\n",
    "top_50_features = sorted_features[:50]\n",
    "for rank, (feature, chi2_stat) in enumerate(top_50_features, start=1):\n",
    "    print(f\"Rank {rank}: {feature} - Chi-square statistic: {chi2_stat}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Redundancy Maximum Relevance (MRMR) Algorithm\n",
    "top 50 important feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n",
      "2\n",
      "155\n",
      "3\n",
      "128\n",
      "4\n",
      "130\n",
      "5\n",
      "129\n",
      "6\n",
      "138\n",
      "7\n",
      "149\n",
      "8\n",
      "150\n",
      "9\n",
      "160\n",
      "10\n",
      "144\n",
      "11\n",
      "134\n",
      "12\n",
      "135\n",
      "13\n",
      "147\n",
      "14\n",
      "158\n",
      "15\n",
      "186\n",
      "16\n",
      "151\n",
      "17\n",
      "169\n",
      "18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m y \u001b[38;5;241m=\u001b[39m data[target_column]\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Perform MRMR feature selection\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mmrmr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Print the selected features\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m, in \u001b[0;36mmrmr\u001b[1;34m(features, y, k)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, relevance \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(mi):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m selected_features:  \u001b[38;5;66;03m# Use index instead of feature name\u001b[39;00m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# Calculate redundancy\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m         red \u001b[38;5;241m=\u001b[39m \u001b[43mredundancy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;66;03m# Calculate MRMR\u001b[39;00m\n\u001b[0;32m     64\u001b[0m         mrmr_score \u001b[38;5;241m=\u001b[39m relevance \u001b[38;5;241m-\u001b[39m red\n",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m, in \u001b[0;36mredundancy\u001b[1;34m(features, selected_features, y)\u001b[0m\n\u001b[0;32m     23\u001b[0m feature_j \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcolumns[selected_features[j]]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate mutual information between the selected features\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m mi \u001b[38;5;241m=\u001b[39m \u001b[43mmutual_info_classif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_j\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Assign mutual information score to the corresponding position in pairwise_mi\u001b[39;00m\n\u001b[0;32m     29\u001b[0m pairwise_mi[i][j] \u001b[38;5;241m=\u001b[39m mi[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming mutual_info_classif returns a 1D array\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:493\u001b[0m, in \u001b[0;36mmutual_info_classif\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate mutual information for a discrete target variable.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03mMutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;124;03m       of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    492\u001b[0m check_classification_targets(y)\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_estimate_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:307\u001b[0m, in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    300\u001b[0m     y \u001b[38;5;241m=\u001b[39m scale(y, with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    301\u001b[0m     y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;241m1e-10\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(y)))\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;241m*\u001b[39m rng\u001b[38;5;241m.\u001b[39mstandard_normal(size\u001b[38;5;241m=\u001b[39mn_samples)\n\u001b[0;32m    305\u001b[0m     )\n\u001b[1;32m--> 307\u001b[0m mi \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_compute_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_feature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_iterate_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(mi)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:308\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    300\u001b[0m     y \u001b[38;5;241m=\u001b[39m scale(y, with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    301\u001b[0m     y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;241m1e-10\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(y)))\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;241m*\u001b[39m rng\u001b[38;5;241m.\u001b[39mstandard_normal(size\u001b[38;5;241m=\u001b[39mn_samples)\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    307\u001b[0m mi \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 308\u001b[0m     \u001b[43m_compute_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, discrete_feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(_iterate_columns(X), discrete_mask)\n\u001b[0;32m    310\u001b[0m ]\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(mi)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:166\u001b[0m, in \u001b[0;36m_compute_mi\u001b[1;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compute_mi_cd(y, x, n_neighbors)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x_discrete \u001b[38;5;129;01mand\u001b[39;00m y_discrete:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compute_mi_cd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compute_mi_cc(x, y, n_neighbors)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:128\u001b[0m, in \u001b[0;36m_compute_mi_cd\u001b[1;34m(c, d, n_neighbors)\u001b[0m\n\u001b[0;32m    126\u001b[0m nn\u001b[38;5;241m.\u001b[39mset_params(n_neighbors\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m    127\u001b[0m nn\u001b[38;5;241m.\u001b[39mfit(c[mask])\n\u001b[1;32m--> 128\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    129\u001b[0m radius[mask] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnextafter(r[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    130\u001b[0m k_all[mask] \u001b[38;5;241m=\u001b[39m k\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:912\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    906\u001b[0m sample_mask \u001b[38;5;241m=\u001b[39m neigh_ind \u001b[38;5;241m!=\u001b[39m sample_range\n\u001b[0;32m    908\u001b[0m \u001b[38;5;66;03m# Corner case: When the number of duplicates are more\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# than the number of neighbors, the first NN will not\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;66;03m# be the sample, but a duplicate.\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# In that case mask the first duplicate.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m dup_gr_nbrs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m sample_mask[:, \u001b[38;5;241m0\u001b[39m][dup_gr_nbrs] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    914\u001b[0m neigh_ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(neigh_ind[sample_mask], (n_queries, n_neighbors \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:2504\u001b[0m, in \u001b[0;36mall\u001b[1;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_all_dispatcher)\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2424\u001b[0m \u001b[38;5;124;03m    Test whether all array elements along a given axis evaluate to True.\u001b[39;00m\n\u001b[0;32m   2425\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2502\u001b[0m \n\u001b[0;32m   2503\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_and\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def mutual_information(X, y):\n",
    "    \"\"\"\n",
    "    Calculate mutual information between each feature and target variable.\n",
    "    \"\"\"\n",
    "    return mutual_info_classif(X, y)\n",
    "\n",
    "def redundancy(features, selected_features, y):\n",
    "    \"\"\"\n",
    "    Calculate redundancy between features.\n",
    "    \"\"\"\n",
    "    n_features = len(selected_features)\n",
    "    pairwise_mi = np.zeros((n_features, n_features))\n",
    "    \n",
    "    # Calculate mutual information between pairs of selected features\n",
    "    for i in range(n_features):\n",
    "        for j in range(i+1, n_features):\n",
    "            # Get the names of the selected features\n",
    "            feature_i = features.columns[selected_features[i]]\n",
    "            feature_j = features.columns[selected_features[j]]\n",
    "            \n",
    "            # Calculate mutual information between the selected features\n",
    "            mi = mutual_info_classif(features[[feature_i, feature_j]], y)\n",
    "            \n",
    "            # Assign mutual information score to the corresponding position in pairwise_mi\n",
    "            pairwise_mi[i][j] = mi[0]  # Assuming mutual_info_classif returns a 1D array\n",
    "            \n",
    "    # Calculate average mutual information\n",
    "    avg_mi = np.mean(pairwise_mi)\n",
    "    \n",
    "    return avg_mi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mrmr(features, y, k=50):\n",
    "    \"\"\"\n",
    "    Minimum Redundancy Maximum Relevance (MRMR) algorithm.\n",
    "    \"\"\"\n",
    "    # Initialize selected features list\n",
    "    selected_features = []\n",
    "    \n",
    "    # Calculate mutual information between each feature and the target variable\n",
    "    mi = mutual_information(features, y)\n",
    "    \n",
    "    # Select the feature with maximum mutual information\n",
    "    selected_features.append(np.argmax(mi))\n",
    "    \n",
    "    # Repeat until k features are selected\n",
    "    while len(selected_features) < k:\n",
    "        max_mrmr = -np.inf\n",
    "        best_feature = None\n",
    "        \n",
    "        # Calculate MRMR for each feature\n",
    "        for i, relevance in enumerate(mi):\n",
    "            if i not in selected_features:  # Use index instead of feature name\n",
    "                # Calculate redundancy\n",
    "                red = redundancy(features, selected_features + [i], y)\n",
    "                \n",
    "                # Calculate MRMR\n",
    "                mrmr_score = relevance - red\n",
    "                \n",
    "                # Update best feature if higher MRMR score found\n",
    "                if mrmr_score > max_mrmr:\n",
    "                    max_mrmr = mrmr_score\n",
    "                    best_feature = i\n",
    "        \n",
    "        # Add best feature index to selected features\n",
    "        print(best_feature)\n",
    "        selected_features.append(best_feature)\n",
    "        print(len(selected_features))\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# Load your data from the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('normalized_data.csv')\n",
    "\n",
    "clmn = data.columns\n",
    "\n",
    "# Assuming your target variable is 'target_column' and your feature columns are in 'feature_columns'\n",
    "target_column = clmn[len(clmn)-1]\n",
    "feature_columns = clmn[:-1]\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data[feature_columns]\n",
    "y = data[target_column]\n",
    "\n",
    "# Perform MRMR feature selection\n",
    "selected_features = mrmr(X, y)\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReliefF algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances: [-0.00704204 -0.01149947 -0.01366499 -0.01217902 -0.00852051 -0.00893852\n",
      " -0.01228927 -0.01049654 -0.00797861 -0.00928189 -0.01175768 -0.01155522\n",
      " -0.00912466 -0.01069516 -0.00856993 -0.00928117 -0.00957907 -0.01000954\n",
      " -0.01132647 -0.01050897 -0.01143534 -0.01174366 -0.00875001 -0.01053138\n",
      " -0.01049138 -0.01054642 -0.00976267 -0.00980463 -0.00996674 -0.01160576\n",
      " -0.01012901 -0.01031776 -0.00069662 -0.00143819 -0.00195739 -0.00159181\n",
      " -0.00076144 -0.00082041 -0.00244627 -0.00238105 -0.00091375 -0.00105046\n",
      " -0.00168098 -0.00099672 -0.00146478 -0.00120024 -0.00096174 -0.00102483\n",
      " -0.00079588 -0.00047091 -0.00082706 -0.00084208 -0.00075243 -0.00086936\n",
      " -0.00116625 -0.00087823 -0.00079293 -0.00121339 -0.00092263 -0.0014026\n",
      " -0.00105238 -0.00121307 -0.00133097 -0.00059777 -0.00578563 -0.00958125\n",
      " -0.01219406 -0.01072876 -0.00745351 -0.00779942 -0.01135858 -0.00980321\n",
      " -0.00730634 -0.00846252 -0.01093459 -0.01061991 -0.00806719 -0.00959365\n",
      " -0.00746782 -0.00834957 -0.00876764 -0.00935748 -0.0104073  -0.0095654\n",
      " -0.01078617 -0.01093839 -0.00846861 -0.00964438 -0.00929912 -0.00909571\n",
      " -0.00879251 -0.00869875 -0.00854166 -0.01005731 -0.00873037 -0.01003443\n",
      " -0.00321259 -0.0052308  -0.00594978 -0.00560251 -0.0038763  -0.00333223\n",
      " -0.00642491 -0.00697163 -0.00370778 -0.00386332 -0.00536573 -0.00431342\n",
      " -0.0048372  -0.00419687 -0.0039171  -0.00386154 -0.00449258 -0.00295694\n",
      " -0.0038971  -0.00408214 -0.00387355 -0.00414519 -0.0031687  -0.00393462\n",
      " -0.00344781 -0.00478109 -0.00381724 -0.00477966 -0.00464459 -0.00574712\n",
      " -0.00544526 -0.00516425 -0.00783633 -0.0168512  -0.01608092 -0.01399234\n",
      " -0.00908031 -0.00796712 -0.01596409 -0.01289298 -0.01144364 -0.01230251\n",
      " -0.01440187 -0.01298482 -0.01567475 -0.01745465 -0.01379353 -0.01448684\n",
      " -0.0115423  -0.00916046 -0.0148431  -0.01430571 -0.01389071 -0.01265251\n",
      " -0.01021799 -0.00931762 -0.00828267 -0.00933212 -0.00735351 -0.00712725\n",
      " -0.00776362 -0.01952804 -0.01754815 -0.01284664 -0.01493982 -0.01662517\n",
      " -0.01755095 -0.0192917  -0.01715232 -0.01567602 -0.0145529  -0.0172094\n",
      " -0.00797285 -0.00903995 -0.01376798 -0.0147094  -0.00966892 -0.0103741\n",
      " -0.00916884 -0.01033163 -0.0120204  -0.01252827 -0.01413225 -0.01235909\n",
      " -0.01377469 -0.01472794 -0.00996893 -0.01347671 -0.01279981 -0.00952999\n",
      " -0.01157339 -0.01082684 -0.01119519 -0.00828069 -0.01627266 -0.01721277\n",
      " -0.00749267 -0.00611574 -0.00487192 -0.00505925 -0.00583104 -0.00642602\n",
      " -0.00522212 -0.00688334 -0.00422805 -0.00510435 -0.00398627 -0.00333674\n",
      " -0.00349856 -0.00319152 -0.00217361 -0.00266266 -0.00284577 -0.00172582\n",
      " -0.00180416 -0.00224728 -0.00211583 -0.002806   -0.0024024  -0.00243451\n",
      " -0.00321207 -0.00380219 -0.00344919 -0.00357722 -0.002385   -0.00321808\n",
      " -0.00634237 -0.00890161 -0.00538348 -0.00493295 -0.00240429 -0.00433912\n",
      " -0.0038715  -0.00346849 -0.00317105 -0.00266045 -0.00313224 -0.00289441\n",
      " -0.00155472 -0.00141092 -0.00166745 -0.00126876 -0.00069635 -0.00140257\n",
      " -0.00049912 -0.00028114 -0.00051263 -0.00055692 -0.00047213 -0.00072476\n",
      " -0.00080056 -0.00080408 -0.00115346 -0.00181672 -0.00158137 -0.00179003\n",
      " -0.00077148 -0.00083951 -0.00482911 -0.00172457 -0.00069662 -0.00143818\n",
      " -0.00195735 -0.00159182 -0.00076143 -0.0008204  -0.00244629 -0.0023811\n",
      " -0.00091386 -0.00105049 -0.00168111 -0.00099674 -0.00146477 -0.00120022\n",
      " -0.00096173 -0.00102481 -0.00079589 -0.00047091 -0.00082706 -0.00084208\n",
      " -0.00075242 -0.00086936 -0.00116663 -0.00087826 -0.00079293 -0.00121342\n",
      " -0.00092263 -0.00140263 -0.00105218 -0.0012131  -0.00133098 -0.00059778\n",
      " -0.00491457 -0.00592754 -0.00760457 -0.0073524  -0.00686476 -0.00474628\n",
      " -0.00581373 -0.00742417 -0.00751052 -0.006979   -0.00845833 -0.00517841\n",
      " -0.00519148 -0.00673675 -0.00730742 -0.00731055 -0.00533148 -0.00733679\n",
      " -0.01016813 -0.00565719 -0.00921863 -0.0082409  -0.00384291 -0.00704795\n",
      " -0.00490502 -0.00549833 -0.00690807 -0.00479823 -0.0028963  -0.00310346\n",
      " -0.00947272 -0.01022553 -0.01252045 -0.01146688 -0.00911985 -0.00889823\n",
      " -0.00837968 -0.01123298 -0.00985974 -0.01300183 -0.00975165 -0.00966543\n",
      " -0.00921827 -0.01139618 -0.00860593 -0.00704522 -0.00815302 -0.00861701\n",
      " -0.01678926 -0.01408435 -0.00840226 -0.00976578 -0.01090878 -0.01077572\n",
      " -0.00908951 -0.00731565 -0.00820894 -0.01108854 -0.00813673 -0.00891666\n",
      " -0.01484609 -0.01933836 -0.01060126 -0.02594354 -0.0009857  -0.00152289\n",
      " -0.00743155 -0.00260385 -0.00103469 -0.00105975 -0.002799   -0.00258926\n",
      " -0.00099318 -0.00200768 -0.00324138 -0.0030246  -0.00208007 -0.00195898\n",
      " -0.00229295 -0.00182648 -0.00137693 -0.00282355 -0.00189809 -0.00176442\n",
      " -0.00248718 -0.00212437 -0.00267244 -0.00219845 -0.00261347 -0.0024369\n",
      " -0.00274246 -0.00191361 -0.00340705 -0.00337595 -0.00151626 -0.00105509]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ReliefF:\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.feature_importances_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize feature importances\n",
    "        n_features = X.shape[1]\n",
    "        self.feature_importances_ = np.zeros(n_features)\n",
    "        \n",
    "        # Normalize feature values\n",
    "        X_normalized = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "        \n",
    "        # Iterate through each instance\n",
    "        for i in range(X.shape[0]):\n",
    "            instance = X_normalized[i]\n",
    "            nearest_same = None\n",
    "            nearest_diff = None\n",
    "            same_class_distance = float('inf')\n",
    "            diff_class_distance = float('inf')\n",
    "            \n",
    "            # Find nearest instances of the same and different classes\n",
    "            for j in range(X.shape[0]):\n",
    "                if i != j:\n",
    "                    distance = np.linalg.norm(instance - X_normalized[j])\n",
    "                    if y[i] == y[j]:  # Same class\n",
    "                        if distance < same_class_distance:\n",
    "                            nearest_same = X_normalized[j]\n",
    "                            same_class_distance = distance\n",
    "                    else:  # Different class\n",
    "                        if distance < diff_class_distance:\n",
    "                            nearest_diff = X_normalized[j]\n",
    "                            diff_class_distance = distance\n",
    "            \n",
    "            # Update feature importances based on the differences\n",
    "            self.feature_importances_ += np.abs(instance - nearest_same) - np.abs(instance - nearest_diff)\n",
    "        \n",
    "        # Average feature importances over all instances\n",
    "        self.feature_importances_ /= X.shape[0]\n",
    "    \n",
    "    def transform(self, X, threshold=None):\n",
    "        if threshold is None:\n",
    "            return X[:, self.feature_importances_.argsort()[::-1]]\n",
    "        else:\n",
    "            return X[:, self.feature_importances_ >= threshold]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X is your feature matrix and y is your target vector\n",
    "\n",
    "# Initialize and fit the ReliefF model\n",
    "\n",
    "# Load your data from the CSV file into a pandas DataFrame\n",
    "\n",
    "data = pd.read_csv('normalized_data.csv')\n",
    "\n",
    "# Assuming your target variable is the last column and your feature columns are all other columns\n",
    "X = data.iloc[:, :-1].values  # Convert DataFrame to NumPy array\n",
    "y = data.iloc[:, -1].values    # Convert DataFrame to NumPy array\n",
    "\n",
    "# Initialize and fit the ReliefF model\n",
    "relieff = ReliefF()\n",
    "relieff.fit(X, y)\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = relieff.feature_importances_\n",
    "print(\"Feature importances:\", feature_importances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 50 according to this algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 features:\n",
      "Feature 241: Importance = -0.0003\n",
      "Feature 49: Importance = -0.0005\n",
      "Feature 273: Importance = -0.0005\n",
      "Feature 244: Importance = -0.0005\n",
      "Feature 240: Importance = -0.0005\n",
      "Feature 242: Importance = -0.0005\n",
      "Feature 243: Importance = -0.0006\n",
      "Feature 63: Importance = -0.0006\n",
      "Feature 287: Importance = -0.0006\n",
      "Feature 238: Importance = -0.0007\n",
      "Feature 32: Importance = -0.0007\n",
      "Feature 256: Importance = -0.0007\n",
      "Feature 245: Importance = -0.0007\n",
      "Feature 276: Importance = -0.0008\n",
      "Feature 52: Importance = -0.0008\n",
      "Feature 260: Importance = -0.0008\n",
      "Feature 36: Importance = -0.0008\n",
      "Feature 252: Importance = -0.0008\n",
      "Feature 56: Importance = -0.0008\n",
      "Feature 280: Importance = -0.0008\n",
      "Feature 48: Importance = -0.0008\n",
      "Feature 272: Importance = -0.0008\n",
      "Feature 246: Importance = -0.0008\n",
      "Feature 247: Importance = -0.0008\n",
      "Feature 261: Importance = -0.0008\n",
      "Feature 37: Importance = -0.0008\n",
      "Feature 274: Importance = -0.0008\n",
      "Feature 50: Importance = -0.0008\n",
      "Feature 253: Importance = -0.0008\n",
      "Feature 275: Importance = -0.0008\n",
      "Feature 51: Importance = -0.0008\n",
      "Feature 277: Importance = -0.0009\n",
      "Feature 53: Importance = -0.0009\n",
      "Feature 55: Importance = -0.0009\n",
      "Feature 279: Importance = -0.0009\n",
      "Feature 40: Importance = -0.0009\n",
      "Feature 264: Importance = -0.0009\n",
      "Feature 282: Importance = -0.0009\n",
      "Feature 58: Importance = -0.0009\n",
      "Feature 270: Importance = -0.0010\n",
      "Feature 46: Importance = -0.0010\n",
      "Feature 352: Importance = -0.0010\n",
      "Feature 360: Importance = -0.0010\n",
      "Feature 43: Importance = -0.0010\n",
      "Feature 267: Importance = -0.0010\n",
      "Feature 271: Importance = -0.0010\n",
      "Feature 47: Importance = -0.0010\n",
      "Feature 356: Importance = -0.0010\n",
      "Feature 41: Importance = -0.0011\n",
      "Feature 265: Importance = -0.0011\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of features sorted by importance (in descending order)\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Get the sorted feature importances\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "\n",
    "# Print the top N features and their importances\n",
    "N = 50  # Number of top features to display\n",
    "print(\"Top {} features:\".format(N))\n",
    "for i in range(N):\n",
    "    print(\"Feature {}: Importance = {:.4f}\".format(sorted_indices[i], sorted_importances[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
